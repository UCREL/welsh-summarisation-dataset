{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "welsh_summariser.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "Jn41SqGn74Q7",
        "9iEeBseYxDIN",
        "fK_kmPQFx3SA"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPITUoM3hEcsLWCvTBS65MQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IgnatiusEzeani/welsh-text-summarizer/blob/main/welsh_summariser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn41SqGn74Q7"
      },
      "source": [
        "## Setting up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72-zM95yvxg0"
      },
      "source": [
        "### a. Installing and importing required Python libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEGPLTpuQHJU"
      },
      "source": [
        "## install dependencies\n",
        "def setup(dependencies):\n",
        " # create the requirement file with the dependencies\n",
        "  with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(dependencies))\n",
        "  !pip install -r requirements.txt\n",
        "  def install_java():\n",
        "    !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"   #set environment variable\n",
        "    !java -version       #check java version\n",
        "\n",
        "setup(dependencies = [\"rouge\", \"rouge-metric\", \"lexrank\", \"summa\", \"fasttext\"])\n",
        "\n",
        "# load setup.py\n",
        "!wget https://raw.githubusercontent.com/IgnatiusEzeani/summariser/master/setup.py\n",
        "\n",
        "# load rouge 2.0 eval jar file\n",
        "!wget https://github.com/IgnatiusEzeani/summariser/raw/master/rouge2-1.2.2.jar\n",
        "!wget https://raw.githubusercontent.com/IgnatiusEzeani/summariser/master/rouge.properties\n",
        "\n",
        "%run setup.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iEeBseYxDIN"
      },
      "source": [
        "## Get Articles and Summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rahWSemxXEm"
      },
      "source": [
        "### a. Extract Wiki articles and summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiXFYTp2Z4d3"
      },
      "source": [
        "# Extract articles and summaries\n",
        "data_file = \"welsh_wiki_articles.json\"\n",
        "if not os.path.exists(data_file):\n",
        "  # Download and unzip data.zip\n",
        "  !wget https://github.com/IgnatiusEzeani/summariser/raw/master/data.zip\n",
        "  unzip(\"data.zip\")\n",
        "\n",
        "  folder_path = \"data/html\"\n",
        "  welsh_wiki_articles={}\n",
        "  for fname in os.listdir(folder_path):\n",
        "      filepath = os.path.join(folder_path, fname)\n",
        "      html = open(filepath, \"r\", encoding=\"utf8\").read()\n",
        "      soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "      text = soup.get_text()\n",
        "      #extract row 'fileId', 'title', 'article' and 'summary'\n",
        "      fileId = getfileID(fname)\n",
        "      headings = ['title', 'text', 'summary']\n",
        "      title, article, summary = [\n",
        "        extract_text(text, f\"begin {item}\", f\"end {item}\") for item in headings]\n",
        "      welsh_wiki_articles[fileId] = {\"fileId\":f\"'{fileId}'\", \"title\":title,\n",
        "                                     \"article\":article, \"summary\":summary}\n",
        "  # Store in a json file for future use\n",
        "  welsh_wiki_articles_json_dump = json.dumps(welsh_wiki_articles)\n",
        "  with open(data_file, \"w\", encoding = \"ISO-8859-1\") as jsonfile:\n",
        "    jsonfile.write(welsh_wiki_articles_json_dump)\n",
        "    print(f\"{data_file} created!\")\n",
        "  \n",
        "  # Remove the data folder\n",
        "  shutil.rmtree(\"data\")\n",
        "else:\n",
        "  print(f\"{data_file} already exists\")\n",
        "\n",
        "# Ensure that \"welsh_wiki_articles.json\" is available\n",
        "wiki_articles_summaries_df = \\\n",
        "      pd.io.json.read_json(data_file, orient='index').sort_index()\n",
        "wiki_articles_summaries_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK_kmPQFx3SA"
      },
      "source": [
        "### b. Upload human summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJS6oVZzv9wO"
      },
      "source": [
        "# Upload and extract Wiki articles and summaries\n",
        "# !wget https://raw.githubusercontent.com/IgnatiusEzeani/summariser/master/acc_prelim.csv\n",
        "# human_summaries_prelim_df = pd.read_csv('acc_prelim.csv', encoding=\"ISO-8859-1\")\n",
        "# human_summaries_prelim_df.dropna(subset=['Crynodeb (tua 250 gair)'], inplace=True)\n",
        "# print(len(human_summaries_prelim_df))\n",
        "# human_summaries_prelim_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgWZIF1sayhr"
      },
      "source": [
        "# Upload and extract Wiki articles and summaries\n",
        "# import pandas as pd\n",
        "!wget https://github.com/IgnatiusEzeani/summariser/raw/master/acc_data.csv\n",
        "human_summaries_df = pd.read_csv('acc_data.csv', encoding=\"ISO-8859-1\")\n",
        "human_summaries_df.dropna(subset=['Summary'], inplace=True)\n",
        "# human_summaries_df.dropna(how='all', axis=1, inplace=True)\n",
        "print(len(human_summaries_df))\n",
        "# human_summaries_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMSFfg5Y7E5W"
      },
      "source": [
        "## Run Baseline Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBUhL6fI7V0n"
      },
      "source": [
        "### 1. BottomLine model\n",
        "- This model basically takes the top sentence i.e. the top sentence from each of the Wiki article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LY6Q8HIX34C"
      },
      "source": [
        "%run setup.py\n",
        "bottomline_summaries_df = summarize(wiki_articles_summaries_df, \"bottomline\")\n",
        "# bottomline_summaries_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMOAOSCfH2BB"
      },
      "source": [
        "### 2. LexRank model\n",
        "Uses lexrank (https://pypi.org/project/lexrank/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8WFWmf1Zr78"
      },
      "source": [
        "lexrank_summaries_df = summarize(wiki_articles_summaries_df, \"lexrank\")\n",
        "# lexrank_summaries_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqqWRmMsMTjs"
      },
      "source": [
        "### 3. TextRank model\n",
        "Uses `summa` (https://pypi.org/project/summa/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKkUiPgqatdl"
      },
      "source": [
        "textrank_summaries_df = summarize(wiki_articles_summaries_df, \"textrank\")\n",
        "# textrank_summaries_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYTDrJcqZW_t"
      },
      "source": [
        "## Run Topline Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syp0ZETUaPoq"
      },
      "source": [
        "### 1. TfIdf model\n",
        "Uses `sklearn`'s [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and [`cosine_similarity`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO5r8iPlY_Hg"
      },
      "source": [
        "tfidf_summaries_df = summarize(wiki_articles_summaries_df, \"tfidf\")\n",
        "# tfidf_summaries_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV6IiBQytRZm"
      },
      "source": [
        "### 2. Word Embeddings (FastText)\n",
        "Source [FastText Library for efficient text classification and representation learning](https://fasttext.cc/docs/en/crawl-vectors.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaKl-KpChm4f"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.cy.300.bin.gz\n",
        "unzip('cc.cy.300.bin.gz')\n",
        "fasttext_model = fasttext.load_model('cc.cy.300.bin')\n",
        "fasttext_summaries_df = summarize(wiki_articles_summaries_df, \"fasttext\",\n",
        "                                                           fasttext_model)\n",
        "# fasttext_summaries_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfanEU4V1GQv"
      },
      "source": [
        "### 3. Word Embeddings (WNLT)\n",
        "Source: Pretrained model from [Welsh Natural Language Technology](https://datainnovation.cardiff.ac.uk/is/wecy/access.html) resources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rYXnqTHQE77"
      },
      "source": [
        "!wget https://datainnovation.cardiff.ac.uk/is/wecy/files/FastText_WNLT_SkipGram.zip\n",
        "unzip(\"FastText_WNLT_SkipGram.zip\")\n",
        "\n",
        "# Loading in FastText_WNLT_SkipGram\n",
        "wnlt_dir = \"./FastText_WNLT_SkipGram/FastText_WNLT_SkipGram\"\n",
        "wnlt_model = gensim.models.FastText.load(os.path.join(wnlt_dir, 'skipgram_subword_model.model'))\n",
        "wnlt_summaries_df = summarize(wiki_articles_summaries_df, \"wnlt\", wnlt_model)\n",
        "wnlt_summaries_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGBtXogNczfF"
      },
      "source": [
        "## Run Model Evaluation\n",
        "Uses `rouge-2.0` (https://github.com/kavgan/ROUGE-2.0)\n",
        "\n",
        "### Generating reference and system summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIYCQ_9BNwBq"
      },
      "source": [
        "# download and unzip system summaries\n",
        "!wget https://github.com/IgnatiusEzeani/summariser/raw/master/system_summaries.zip\n",
        "zip_file = \"system_summaries.zip\"\n",
        "unzip(zip_file)\n",
        "\n",
        "system_summaries_dir = os.path.join('.',zip_file[:-4],zip_file[:-4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRMI9oxUXcwK"
      },
      "source": [
        "summaries_bottomline_df = pd.read_json(os.path.join(system_summaries_dir,\n",
        "                                      'summaries_bottomline.json'),\n",
        "                                       orient=\"index\")\n",
        "summaries_lexrank_df = pd.read_json(os.path.join(system_summaries_dir,\n",
        "                                      'summaries_lexrank.json'),\n",
        "                                       orient=\"index\")\n",
        "summaries_textrank_df = pd.read_json(os.path.join(system_summaries_dir,\n",
        "                                      'summaries_textrank.json'),\n",
        "                                       orient=\"index\")\n",
        "summaries_tfidf_df = pd.read_json(os.path.join(system_summaries_dir,\n",
        "                                      'summaries_tfidf.json'),\n",
        "                                       orient=\"index\")\n",
        "summaries_fasttext_df = pd.read_json(os.path.join(system_summaries_dir,\n",
        "                                      'summaries_fasttext.json'),\n",
        "                                       orient=\"index\")\n",
        "summaries_wnlt_df = pd.read_json(os.path.join(system_summaries_dir,\n",
        "                                      'summaries_wnlt.json'),\n",
        "                                       orient=\"index\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9-k4QpzUm3a"
      },
      "source": [
        "# Load reference summaries\n",
        "# reference_types = [(human_summaries_df, \"human\"), (wiki_articles_summaries_df, \"wiki\")]\n",
        "reference_types = [(wiki_articles_summaries_df, \"wiki\")]\n",
        "\n",
        "# Set up models for system summaries\n",
        "models = [(\"bottomline\", summaries_bottomline_df),\n",
        "          (\"lexrank\", summaries_lexrank_df),\n",
        "          (\"textrank\", summaries_textrank_df),\n",
        "          (\"tfidf\", summaries_tfidf_df),\n",
        "          (\"fasttext\", summaries_fasttext_df),\n",
        "          (\"wnlt\", summaries_wnlt_df)\n",
        "          ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_references(reference_types)"
      ],
      "metadata": {
        "id": "hXpdak0fDHm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_model_summaries(models)"
      ],
      "metadata": {
        "id": "JpvK0OEuDJoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssSk8RHexXuL"
      },
      "source": [
        "# run evaluation\n",
        "!java -jar \"rouge2-1.2.2.jar\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GVoGTAJ1lQm"
      },
      "source": [
        "model_types = ['BOTTOMLINE1.TXT', 'TEXTRANK1.TXT', 'LEXRANK1.TXT',\n",
        "               'TFIDF1.TXT', 'FASTTEXT1.TXT', 'WNLT1.TXT']\n",
        "rouge_types = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'ROUGE-SU4']\n",
        "eval_results_df = pd.read_csv('results.csv', encoding=\"ISO-8859-1\")\n",
        "print(f\"Models+Metric   Pre\\t    Rec     F-Meas\")\n",
        "for model in model_types:\n",
        "  print(f\"{model}:\\n\", end=\" \")\n",
        "  for metric in rouge_types:\n",
        "    print(f\"{metric:10s}\", end=\"\")\n",
        "    model_result_df = eval_results_df.loc[np.where(\n",
        "                              (eval_results_df['System Name']==model) & \n",
        "                              (eval_results_df['ROUGE-Type']==metric))]\n",
        "    res=np.mean(model_result_df[['Avg_Recall', 'Avg_Precision',\t'Avg_F-Score']])*100\n",
        "    print(f\"&\\t{res[1]:05.2f}  &  {res[0]:05.2f}  &  {res[2]:05.2f} \\\\\\\\\", end=\"\\n \")\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqyFLdKvyVLE"
      },
      "source": [
        "**Human Summaries**\n",
        "```\n",
        "Models+Metric   Pre\t    Rec     F-Meas\n",
        "BOTTOMLINE1.TXT:\n",
        " ROUGE-1   &\t70.52  &  06.34  &  11.15 \\\\\n",
        " ROUGE-2   &\t42.20  &  03.71  &  06.53 \\\\\n",
        " ROUGE-L   &\t61.69  &  08.25  &  13.94 \\\\\n",
        " ROUGE-SU4 &\t44.62  &  03.77  &  06.65 \\\\\n",
        "TEXTRANK1.TXT:\n",
        " ROUGE-1   &\t27.60  &  70.17  &  36.73 \\\\\n",
        " ROUGE-2   &\t15.90  &  39.89  &  21.04 \\\\\n",
        " ROUGE-L   &\t26.85  &  56.05  &  33.97 \\\\\n",
        " ROUGE-SU4 &\t17.36  &  42.82  &  22.83 \\\\ \n",
        "LEXRANK1.TXT:\n",
        " ROUGE-1   &\t30.10  &  67.44  &  38.11 \\\\\n",
        " ROUGE-2   &\t17.09  &  37.53  &  21.43 \\\\\n",
        " ROUGE-L   &\t27.85  &  53.08  &  33.96 \\\\\n",
        " ROUGE-SU4 &\t18.78  &  40.42  &  23.38 \\\\\n",
        " \n",
        "TFIDF1.TXT:\n",
        " ROUGE-1   &\t29.83  &  67.59  &  37.84 \\\\\n",
        " ROUGE-2   &\t17.05  &  37.91  &  21.41 \\\\\n",
        " ROUGE-L   &\t28.43  &  52.95  &  34.31 \\\\\n",
        " ROUGE-SU4 &\t18.76  &  40.89  &  23.39 \\\\ \n",
        "FASTTEXT1.TXT:\n",
        " ROUGE-1   &\t29.37  &  68.68  &  37.75 \\\\\n",
        " ROUGE-2   &\t16.72  &  38.52  &  21.35 \\\\\n",
        " ROUGE-L   &\t27.74  &  55.11  &  34.35 \\\\\n",
        " ROUGE-SU4 &\t18.37  &  41.48  &  23.27 \\\\\n",
        "WNLT1.TXT:\n",
        " ROUGE-1   &\t28.66  &  70.78  &  37.50 \\\\\n",
        " ROUGE-2   &\t16.48  &  40.04  &  21.37 \\\\\n",
        " ROUGE-L   &\t27.24  &  57.13  &  34.35 \\\\\n",
        " ROUGE-SU4 &\t17.99  &  43.01  &  23.21 \\\\\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wiki Summaries**\n",
        "```\n",
        "Models+Metric   Pre\t    Rec     F-Meas\n",
        "BOTTOMLINE1.TXT:\n",
        " ROUGE-1   &\t99.51  &  24.45  &  34.07 \\\\\n",
        " ROUGE-2   &\t99.50  &  23.79  &  33.17 \\\\\n",
        " ROUGE-L   &\t99.53  &  29.03  &  40.26 \\\\\n",
        " ROUGE-SU4 &\t99.48  &  23.11  &  32.23 \\\\\n",
        "TEXTRANK1.TXT:\n",
        " ROUGE-1   &\t21.12  &  81.91  &  29.56 \\\\\n",
        " ROUGE-2   &\t17.98  &  64.62  &  24.61 \\\\\n",
        " ROUGE-L   &\t24.47  &  73.78  &  33.28 \\\\\n",
        " ROUGE-SU4 &\t18.67  &  66.19  &  25.40 \\\\\n",
        "LEXRANK1.TXT:\n",
        " ROUGE-1   &\t22.90  &  79.31  &  30.98 \\\\\n",
        " ROUGE-2   &\t19.04  &  60.95  &  25.07 \\\\\n",
        " ROUGE-L   &\t25.56  &  70.82  &  33.81 \\\\\n",
        " ROUGE-SU4 &\t19.87  &  62.54  &  25.97 \\\\\n",
        "TFIDF1.TXT:\n",
        " ROUGE-1   &\t22.88  &  80.41  &  31.11 \\\\\n",
        " ROUGE-2   &\t19.42  &  63.72  &  25.81 \\\\\n",
        " ROUGE-L   &\t26.44  &  72.38  &  34.93 \\\\\n",
        " ROUGE-SU4 &\t20.21  &  65.16  &  26.66 \\\\\n",
        "FASTTEXT1.TXT:\n",
        " ROUGE-1   &\t22.44  &  80.57  &  30.71 \\\\\n",
        " ROUGE-2   &\t18.87  &  62.82  &  25.20 \\\\\n",
        " ROUGE-L   &\t25.21  &  72.70  &  33.83 \\\\\n",
        " ROUGE-SU4 &\t19.67  &  64.43  &  26.08 \\\\\n",
        "WNLT1.TXT:\n",
        " ROUGE-1   &\t21.73  &  81.80  &  30.19 \\\\\n",
        " ROUGE-2   &\t18.33  &  63.04  &  24.83 \\\\\n",
        " ROUGE-L   &\t24.48  &  73.63  &  33.32 \\\\\n",
        " ROUGE-SU4 &\t19.04  &  64.77  &  25.64 \\\\\n",
        "```"
      ],
      "metadata": {
        "id": "N951_LluZva5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFSqFvwEa1DP"
      },
      "source": [
        "## Playgound"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNnpFygesFGu"
      },
      "source": [
        "human_summaries_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPYAG_H_oAhZ"
      },
      "source": [
        "strnumTo3dgt = lambda x: f\"{int(x):03d}\" #integer num to 3 digits\n",
        "getfileID = lambda fn: strnumTo3dgt(fn.split('_',1)[0]) #extract a file number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpYIvLI_zlBk"
      },
      "source": [
        "from collections import Counter\n",
        "counts = Counter([getfileID(fn) for fn in human_summaries_df[\"Article\"]])\n",
        "len(counts), Counter(counts.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4K3V4eRwSRh"
      },
      "source": [
        "len(human_summaries_df[\"Article\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sorted(counts.keys()))"
      ],
      "metadata": {
        "id": "mKdXXWzGySw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in sorted(counts.keys()):\n",
        "  print(k,end=' ')"
      ],
      "metadata": {
        "id": "AGizK1FvagH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZHT6r65p5BP"
      },
      "source": [
        "def check_summaries(num_summaries):\n",
        "  count_summaries = [getfileID(id) for id, count in counts.items() if count==num_summaries]\n",
        "  count_summarizer = Counter([s.upper().strip() for s in human_summaries_df['Summariser']])\n",
        "  articles = {}\n",
        "  summarisers = {}\n",
        "  for row in range(len(human_summaries_df[\"Article\"])):\n",
        "    article_id = getfileID(human_summaries_df[\"Article\"][row])\n",
        "    if article_id in count_summaries:\n",
        "      summariser = human_summaries_df[\"Summariser\"][row]\n",
        "      articles.setdefault(article_id, []).append((row, summariser))\n",
        "  return articles, count_summarizer\n",
        "  # return {id:len(Counter(summarisers)) for id, summarisers in articles.items()}\n",
        "\n",
        "for k, v in sorted(check_summaries(2)[1].items()):\n",
        "  print(f\"{k}={v}; \", end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJzASu5lnAc8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}